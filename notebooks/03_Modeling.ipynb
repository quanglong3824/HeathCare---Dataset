{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ X√¢y D·ª±ng v√† ƒê√°nh Gi√° M√¥ H√¨nh Machine Learning\n",
    "\n",
    "## M·ª•c ti√™u:\n",
    "- X√¢y d·ª±ng v√† so s√°nh nhi·ªÅu m√¥ h√¨nh ML\n",
    "- T·ªëi ∆∞u h√≥a hyperparameters\n",
    "- ƒê√°nh gi√° hi·ªáu su·∫•t v·ªõi c√°c metrics ph√π h·ª£p\n",
    "- Ph√¢n t√≠ch feature importance\n",
    "- Ch·ªçn m√¥ h√¨nh t·ªët nh·∫•t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "\nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <B637898E-C0C3-3F93-8C08-800EE41A7A5B> /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file)\"]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mXGBoostError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnaive_bayes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GaussianNB\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeClassifier\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LGBMClassifier\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Model Selection v√† Evaluation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"XGBoost: eXtreme Gradient Boosting library.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mContributors: https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracker  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     Booster,\n\u001b[32m     10\u001b[39m     DataIter,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     build_info,\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/tracker.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menum\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IntEnum, unique\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional, Union\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LIB, _check_call, _deprecate_positional_args, make_jcargs\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_family\u001b[39m(addr: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m     13\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get network family from address.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/core.py:295\u001b[39m\n\u001b[32m    291\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\n\u001b[32m    294\u001b[39m \u001b[38;5;66;03m# load the XGBoost library globally\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m _LIB = \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_call\u001b[39m(ret: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    299\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[32m    300\u001b[39m \n\u001b[32m    301\u001b[39m \u001b[33;03m    This function will raise exception when error occurs.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m \u001b[33;03m        return value from API calls\u001b[39;00m\n\u001b[32m    308\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/core.py:257\u001b[39m, in \u001b[36m_load_lib\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib_success:\n\u001b[32m    256\u001b[39m         libname = os.path.basename(lib_paths[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(\n\u001b[32m    258\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[33mXGBoost Library (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlibname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) could not be loaded.\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[33mLikely causes:\u001b[39m\n\u001b[32m    261\u001b[39m \u001b[33m  * OpenMP runtime is not installed\u001b[39m\n\u001b[32m    262\u001b[39m \u001b[33m    - vcomp140.dll or libgomp-1.dll for Windows\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[33m    - libomp.dylib for Mac OSX\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[33m    - libgomp.so for Linux and other UNIX-like OSes\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[33m    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\u001b[39m\n\u001b[32m    266\u001b[39m \n\u001b[32m    267\u001b[39m \u001b[33m  * You are running 32-bit Python on a 64-bit OS\u001b[39m\n\u001b[32m    268\u001b[39m \n\u001b[32m    269\u001b[39m \u001b[33mError message(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_error_list\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    271\u001b[39m         )\n\u001b[32m    272\u001b[39m     _register_log_callback(lib)\n\u001b[32m    274\u001b[39m     libver = _lib_version(lib)\n",
      "\u001b[31mXGBoostError\u001b[39m: \nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <B637898E-C0C3-3F93-8C08-800EE41A7A5B> /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file)\"]\n"
     ]
    }
   ],
   "source": [
    "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Model Selection v√† Evaluation\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report, roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "# C·∫•u h√¨nh hi·ªÉn th·ªã\n",
    "plt.style.use('seaborn-v0_8')\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. T·∫£i D·ªØ Li·ªáu ƒê√£ X·ª≠ L√Ω"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫£i c√°c version d·ªØ li·ªáu kh√°c nhau\n",
    "def load_data(filename):\n",
    "    with open(f'../data/processed/{filename}', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# T·∫£i t·∫•t c·∫£ versions\n",
    "data_original = load_data('data_original.pkl')\n",
    "data_smote = load_data('data_smote.pkl')\n",
    "data_under = load_data('data_under.pkl')\n",
    "data_smoteenn = load_data('data_smoteenn.pkl')\n",
    "\n",
    "print(\"üìä D·ªÆ LI·ªÜU ƒê√É T·∫¢I:\")\n",
    "datasets = {\n",
    "    'Original': data_original,\n",
    "    'SMOTE': data_smote,\n",
    "    'Under Sampling': data_under,\n",
    "    'SMOTEENN': data_smoteenn\n",
    "}\n",
    "\n",
    "for name, data in datasets.items():\n",
    "    X_train, y_train = data['X_train'], data['y_train']\n",
    "    stroke_rate = np.mean(y_train)\n",
    "    print(f\"{name:15} - Train: {X_train.shape[0]:5} samples - Stroke rate: {stroke_rate:.3f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Features: {len(data_original['feature_names'])}\")\n",
    "print(f\"‚úÖ Test samples: {data_original['X_test'].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ƒê·ªãnh Nghƒ©a C√°c M√¥ H√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê·ªãnh nghƒ©a c√°c m√¥ h√¨nh v·ªõi hyperparameters c∆° b·∫£n\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "    'LightGBM': LGBMClassifier(random_state=42, verbose=-1),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "print(f\"ü§ñ ƒê√É ƒê·ªäNH NGHƒ®A {len(models)} M√î H√åNH:\")\n",
    "for i, name in enumerate(models.keys(), 1):\n",
    "    print(f\"{i:2}. {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. H√†m ƒê√°nh Gi√° M√¥ H√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    ƒê√°nh gi√° to√†n di·ªán m·ªôt m√¥ h√¨nh\n",
    "    \"\"\"\n",
    "    # Training\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None\n",
    "    }\n",
    "    \n",
    "    return model, metrics, y_pred, y_pred_proba\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix\"):\n",
    "    \"\"\"\n",
    "    V·∫Ω confusion matrix\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['No Stroke', 'Stroke'],\n",
    "                yticklabels=['No Stroke', 'Stroke'])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curve(y_true, y_pred_proba, title=\"ROC Curve\"):\n",
    "    \"\"\"\n",
    "    V·∫Ω ROC curve\n",
    "    \"\"\"\n",
    "    if y_pred_proba is None:\n",
    "        print(\"Kh√¥ng c√≥ probability predictions ƒë·ªÉ v·∫Ω ROC curve\")\n",
    "        return\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a c√°c h√†m ƒë√°nh gi√° m√¥ h√¨nh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. So S√°nh M√¥ H√¨nh Tr√™n D·ªØ Li·ªáu G·ªëc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S·ª≠ d·ª•ng d·ªØ li·ªáu g·ªëc ƒë·ªÉ so s√°nh ban ƒë·∫ßu\n",
    "X_train = data_original['X_train']\n",
    "X_test = data_original['X_test']\n",
    "y_train = data_original['y_train']\n",
    "y_test = data_original['y_test']\n",
    "\n",
    "print(\"üîÑ ƒêANG TRAINING V√Ä ƒê√ÅNH GI√Å C√ÅC M√î H√åNH...\")\n",
    "print(\"(C√≥ th·ªÉ m·∫•t v√†i ph√∫t)\\n\")\n",
    "\n",
    "# L∆∞u k·∫øt qu·∫£\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"üîÑ Training {name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Cross-validation scores\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "        \n",
    "        # Train v√† evaluate tr√™n test set\n",
    "        trained_model, metrics, y_pred, y_pred_proba = evaluate_model(\n",
    "            model, X_train, X_test, y_train, y_test, name\n",
    "        )\n",
    "        \n",
    "        # L∆∞u k·∫øt qu·∫£\n",
    "        results[name] = {\n",
    "            'cv_auc_mean': cv_scores.mean(),\n",
    "            'cv_auc_std': cv_scores.std(),\n",
    "            'test_metrics': metrics,\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_pred_proba\n",
    "        }\n",
    "        trained_models[name] = trained_model\n",
    "        \n",
    "        print(f\"   ‚úÖ CV AUC: {cv_scores.mean():.3f} (¬±{cv_scores.std():.3f})\")\n",
    "        print(f\"   ‚úÖ Test AUC: {metrics['roc_auc']:.3f if metrics['roc_auc'] else 'N/A'}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n‚úÖ Ho√†n th√†nh training {len(results)} m√¥ h√¨nh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. B·∫£ng So S√°nh K·∫øt Qu·∫£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o b·∫£ng so s√°nh\n",
    "comparison_data = []\n",
    "\n",
    "for name, result in results.items():\n",
    "    metrics = result['test_metrics']\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'CV AUC': f\"{result['cv_auc_mean']:.3f} ¬± {result['cv_auc_std']:.3f}\",\n",
    "        'Test AUC': f\"{metrics['roc_auc']:.3f}\" if metrics['roc_auc'] else 'N/A',\n",
    "        'Accuracy': f\"{metrics['accuracy']:.3f}\",\n",
    "        'Precision': f\"{metrics['precision']:.3f}\",\n",
    "        'Recall': f\"{metrics['recall']:.3f}\",\n",
    "        'F1-Score': f\"{metrics['f1']:.3f}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Test AUC', ascending=False)\n",
    "\n",
    "print(\"üìä B·∫¢NG SO S√ÅNH K·∫æT QU·∫¢ C√ÅC M√î H√åNH:\")\n",
    "print(\"=\" * 100)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# AUC scores\n",
    "auc_scores = [results[name]['test_metrics']['roc_auc'] for name in results.keys() \n",
    "              if results[name]['test_metrics']['roc_auc'] is not None]\n",
    "model_names = [name for name in results.keys() \n",
    "               if results[name]['test_metrics']['roc_auc'] is not None]\n",
    "\n",
    "axes[0,0].barh(model_names, auc_scores, color='skyblue')\n",
    "axes[0,0].set_xlabel('AUC Score')\n",
    "axes[0,0].set_title('Test AUC Scores')\n",
    "axes[0,0].set_xlim(0, 1)\n",
    "\n",
    "# F1 scores\n",
    "f1_scores = [results[name]['test_metrics']['f1'] for name in results.keys()]\n",
    "axes[0,1].barh(list(results.keys()), f1_scores, color='lightgreen')\n",
    "axes[0,1].set_xlabel('F1 Score')\n",
    "axes[0,1].set_title('F1 Scores')\n",
    "axes[0,1].set_xlim(0, 1)\n",
    "\n",
    "# Precision vs Recall\n",
    "precisions = [results[name]['test_metrics']['precision'] for name in results.keys()]\n",
    "recalls = [results[name]['test_metrics']['recall'] for name in results.keys()]\n",
    "\n",
    "axes[1,0].scatter(recalls, precisions, s=100, alpha=0.7)\n",
    "for i, name in enumerate(results.keys()):\n",
    "    axes[1,0].annotate(name, (recalls[i], precisions[i]), \n",
    "                      xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "axes[1,0].set_xlabel('Recall')\n",
    "axes[1,0].set_ylabel('Precision')\n",
    "axes[1,0].set_title('Precision vs Recall')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# CV AUC v·ªõi error bars\n",
    "cv_means = [results[name]['cv_auc_mean'] for name in results.keys()]\n",
    "cv_stds = [results[name]['cv_auc_std'] for name in results.keys()]\n",
    "\n",
    "axes[1,1].barh(list(results.keys()), cv_means, xerr=cv_stds, \n",
    "               color='salmon', alpha=0.7, capsize=5)\n",
    "axes[1,1].set_xlabel('CV AUC Score')\n",
    "axes[1,1].set_title('Cross-Validation AUC Scores')\n",
    "axes[1,1].set_xlim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ph√¢n T√≠ch Top 3 M√¥ H√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch·ªçn top 3 m√¥ h√¨nh d·ª±a tr√™n AUC\n",
    "auc_results = [(name, results[name]['test_metrics']['roc_auc']) \n",
    "               for name in results.keys() \n",
    "               if results[name]['test_metrics']['roc_auc'] is not None]\n",
    "auc_results.sort(key=lambda x: x[1], reverse=True)\n",
    "top_3_models = [name for name, _ in auc_results[:3]]\n",
    "\n",
    "print(f\"üèÜ TOP 3 M√î H√åNH THEO AUC:\")\n",
    "for i, name in enumerate(top_3_models, 1):\n",
    "    auc = results[name]['test_metrics']['roc_auc']\n",
    "    print(f\"{i}. {name}: {auc:.3f}\")\n",
    "\n",
    "# Ph√¢n t√≠ch chi ti·∫øt top 3\n",
    "fig, axes = plt.subplots(len(top_3_models), 3, figsize=(18, 6*len(top_3_models)))\n",
    "if len(top_3_models) == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i, model_name in enumerate(top_3_models):\n",
    "    y_pred = results[model_name]['predictions']\n",
    "    y_pred_proba = results[model_name]['probabilities']\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i,0],\n",
    "                xticklabels=['No Stroke', 'Stroke'],\n",
    "                yticklabels=['No Stroke', 'Stroke'])\n",
    "    axes[i,0].set_title(f'{model_name} - Confusion Matrix')\n",
    "    axes[i,0].set_xlabel('Predicted')\n",
    "    axes[i,0].set_ylabel('Actual')\n",
    "    \n",
    "    # ROC Curve\n",
    "    if y_pred_proba is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        axes[i,1].plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                      label=f'ROC curve (AUC = {auc:.3f})')\n",
    "        axes[i,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        axes[i,1].set_xlim([0.0, 1.0])\n",
    "        axes[i,1].set_ylim([0.0, 1.05])\n",
    "        axes[i,1].set_xlabel('False Positive Rate')\n",
    "        axes[i,1].set_ylabel('True Positive Rate')\n",
    "        axes[i,1].set_title(f'{model_name} - ROC Curve')\n",
    "        axes[i,1].legend(loc=\"lower right\")\n",
    "        axes[i,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Precision-Recall Curve\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "        axes[i,2].plot(recall, precision, color='blue', lw=2)\n",
    "        axes[i,2].set_xlabel('Recall')\n",
    "        axes[i,2].set_ylabel('Precision')\n",
    "        axes[i,2].set_title(f'{model_name} - Precision-Recall Curve')\n",
    "        axes[i,2].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[i,1].text(0.5, 0.5, 'No probability\\npredictions available', \n",
    "                      ha='center', va='center', transform=axes[i,1].transAxes)\n",
    "        axes[i,2].text(0.5, 0.5, 'No probability\\npredictions available', \n",
    "                      ha='center', va='center', transform=axes[i,2].transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification reports\n",
    "print(\"\\nüìã CLASSIFICATION REPORTS:\")\n",
    "print(\"=\" * 80)\n",
    "for model_name in top_3_models:\n",
    "    y_pred = results[model_name]['predictions']\n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(classification_report(y_test, y_pred, target_names=['No Stroke', 'Stroke']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ph√¢n t√≠ch feature importance cho c√°c m√¥ h√¨nh tree-based\n",
    "tree_based_models = ['Random Forest', 'Gradient Boosting', 'XGBoost', 'LightGBM']\n",
    "feature_names = data_original['feature_names']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, model_name in enumerate(tree_based_models):\n",
    "    if model_name in trained_models:\n",
    "        model = trained_models[model_name]\n",
    "        \n",
    "        # L·∫•y feature importance\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            \n",
    "            # T·∫°o DataFrame v√† sort\n",
    "            feature_imp = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'importance': importances\n",
    "            }).sort_values('importance', ascending=True)\n",
    "            \n",
    "            # L·∫•y top 15 features\n",
    "            top_features = feature_imp.tail(15)\n",
    "            \n",
    "            # Plot\n",
    "            axes[i].barh(top_features['feature'], top_features['importance'], \n",
    "                        color='skyblue', alpha=0.8)\n",
    "            axes[i].set_title(f'{model_name} - Top 15 Feature Importance')\n",
    "            axes[i].set_xlabel('Importance')\n",
    "            axes[i].tick_params(axis='y', labelsize=8)\n",
    "            \n",
    "            # In top 10 features\n",
    "            print(f\"\\nüîç {model_name.upper()} - TOP 10 FEATURES:\")\n",
    "            print(\"-\" * 50)\n",
    "            for idx, row in feature_imp.tail(10).iterrows():\n",
    "                print(f\"{row['feature']:30} {row['importance']:.4f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hyperparameter Tuning cho M√¥ H√¨nh T·ªët Nh·∫•t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch·ªçn m√¥ h√¨nh t·ªët nh·∫•t ƒë·ªÉ tune\n",
    "best_model_name = top_3_models[0]\n",
    "print(f\"üéØ HYPERPARAMETER TUNING CHO: {best_model_name}\")\n",
    "\n",
    "# ƒê·ªãnh nghƒ©a parameter grids\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 6, 10],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 6, 10],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'num_leaves': [31, 50, 100],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 6, 10],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    }\n",
    "}\n",
    "\n",
    "if best_model_name in param_grids:\n",
    "    print(f\"üîÑ ƒêang th·ª±c hi·ªán RandomizedSearchCV...\")\n",
    "    \n",
    "    # T·∫°o m√¥ h√¨nh base\n",
    "    base_model = models[best_model_name]\n",
    "    \n",
    "    # RandomizedSearchCV\n",
    "    random_search = RandomizedSearchCV(\n",
    "        base_model,\n",
    "        param_grids[best_model_name],\n",
    "        n_iter=50,  # S·ªë l·∫ßn th·ª≠\n",
    "        cv=5,\n",
    "        scoring='roc_auc',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    # K·∫øt qu·∫£\n",
    "    print(f\"\\n‚úÖ BEST PARAMETERS:\")\n",
    "    for param, value in random_search.best_params_.items():\n",
    "        print(f\"{param:20}: {value}\")\n",
    "    \n",
    "    print(f\"\\nüìä BEST CV SCORE: {random_search.best_score_:.4f}\")\n",
    "    \n",
    "    # ƒê√°nh gi√° m√¥ h√¨nh ƒë√£ tune\n",
    "    best_tuned_model = random_search.best_estimator_\n",
    "    tuned_model, tuned_metrics, tuned_pred, tuned_proba = evaluate_model(\n",
    "        best_tuned_model, X_train, X_test, y_train, y_test, f\"{best_model_name} (Tuned)\"\n",
    "    )\n",
    "    \n",
    "    # So s√°nh v·ªõi m√¥ h√¨nh g·ªëc\n",
    "    original_metrics = results[best_model_name]['test_metrics']\n",
    "    \n",
    "    print(f\"\\nüìà SO S√ÅNH K·∫æT QU·∫¢:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Metric':<15} {'Original':<12} {'Tuned':<12} {'Improvement':<12}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']:\n",
    "        if original_metrics[metric] is not None and tuned_metrics[metric] is not None:\n",
    "            orig_val = original_metrics[metric]\n",
    "            tuned_val = tuned_metrics[metric]\n",
    "            improvement = tuned_val - orig_val\n",
    "            print(f\"{metric:<15} {orig_val:<12.4f} {tuned_val:<12.4f} {improvement:+.4f}\")\n",
    "    \n",
    "    # L∆∞u m√¥ h√¨nh t·ªët nh·∫•t\n",
    "    best_final_model = best_tuned_model\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Kh√¥ng c√≥ parameter grid cho {best_model_name}\")\n",
    "    best_final_model = trained_models[best_model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Th·ª≠ Nghi·ªám v·ªõi D·ªØ Li·ªáu SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ TH·ª∞C NGHI·ªÜM V·ªöI D·ªÆ LI·ªÜU SMOTE:\")\n",
    "\n",
    "# S·ª≠ d·ª•ng d·ªØ li·ªáu SMOTE\n",
    "X_train_smote = data_smote['X_train']\n",
    "y_train_smote = data_smote['y_train']\n",
    "X_test_smote = data_smote['X_test']  # Test set gi·ªØ nguy√™n\n",
    "y_test_smote = data_smote['y_test']\n",
    "\n",
    "print(f\"Train set size: {X_train_smote.shape[0]} (SMOTE)\")\n",
    "print(f\"Stroke ratio: {np.mean(y_train_smote):.3f}\")\n",
    "\n",
    "# Test top 3 models v·ªõi SMOTE data\n",
    "smote_results = {}\n",
    "\n",
    "for model_name in top_3_models:\n",
    "    print(f\"\\nüîÑ Testing {model_name} with SMOTE data...\")\n",
    "    \n",
    "    # T·∫°o m√¥ h√¨nh m·ªõi\n",
    "    model = models[model_name]\n",
    "    \n",
    "    # Train v√† evaluate\n",
    "    trained_model, metrics, y_pred, y_pred_proba = evaluate_model(\n",
    "        model, X_train_smote, X_test_smote, y_train_smote, y_test_smote, \n",
    "        f\"{model_name} (SMOTE)\"\n",
    "    )\n",
    "    \n",
    "    smote_results[model_name] = {\n",
    "        'metrics': metrics,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"   AUC: {metrics['roc_auc']:.4f}\")\n",
    "    print(f\"   F1:  {metrics['f1']:.4f}\")\n",
    "\n",
    "# So s√°nh k·∫øt qu·∫£ Original vs SMOTE\n",
    "print(\"\\nüìä SO S√ÅNH ORIGINAL VS SMOTE:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Model':<20} {'Original AUC':<15} {'SMOTE AUC':<15} {'Original F1':<15} {'SMOTE F1':<15}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_name in top_3_models:\n",
    "    orig_auc = results[model_name]['test_metrics']['roc_auc']\n",
    "    smote_auc = smote_results[model_name]['metrics']['roc_auc']\n",
    "    orig_f1 = results[model_name]['test_metrics']['f1']\n",
    "    smote_f1 = smote_results[model_name]['metrics']['f1']\n",
    "    \n",
    "    print(f\"{model_name:<20} {orig_auc:<15.4f} {smote_auc:<15.4f} {orig_f1:<15.4f} {smote_f1:<15.4f}\")\n",
    "\n",
    "# Visualization comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# AUC comparison\n",
    "model_names = list(top_3_models)\n",
    "original_aucs = [results[name]['test_metrics']['roc_auc'] for name in model_names]\n",
    "smote_aucs = [smote_results[name]['metrics']['roc_auc'] for name in model_names]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, original_aucs, width, label='Original', alpha=0.8, color='skyblue')\n",
    "axes[0].bar(x + width/2, smote_aucs, width, label='SMOTE', alpha=0.8, color='lightgreen')\n",
    "axes[0].set_xlabel('Models')\n",
    "axes[0].set_ylabel('AUC Score')\n",
    "axes[0].set_title('AUC Comparison: Original vs SMOTE')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(model_names, rotation=45)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# F1 comparison\n",
    "original_f1s = [results[name]['test_metrics']['f1'] for name in model_names]\n",
    "smote_f1s = [smote_results[name]['metrics']['f1'] for name in model_names]\n",
    "\n",
    "axes[1].bar(x - width/2, original_f1s, width, label='Original', alpha=0.8, color='salmon')\n",
    "axes[1].bar(x + width/2, smote_f1s, width, label='SMOTE', alpha=0.8, color='gold')\n",
    "axes[1].set_xlabel('Models')\n",
    "axes[1].set_ylabel('F1 Score')\n",
    "axes[1].set_title('F1 Comparison: Original vs SMOTE')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(model_names, rotation=45)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. L∆∞u M√¥ H√¨nh T·ªët Nh·∫•t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X√°c ƒë·ªãnh m√¥ h√¨nh t·ªët nh·∫•t cu·ªëi c√πng\n",
    "print(\"üíæ L∆ØU M√î H√åNH T·ªêT NH·∫§T:\")\n",
    "\n",
    "# So s√°nh t·∫•t c·∫£ c√°c version\n",
    "final_comparison = []\n",
    "\n",
    "# Original models\n",
    "for name in top_3_models:\n",
    "    metrics = results[name]['test_metrics']\n",
    "    final_comparison.append({\n",
    "        'model_name': f\"{name} (Original)\",\n",
    "        'auc': metrics['roc_auc'],\n",
    "        'f1': metrics['f1'],\n",
    "        'precision': metrics['precision'],\n",
    "        'recall': metrics['recall']\n",
    "    })\n",
    "\n",
    "# SMOTE models\n",
    "for name in top_3_models:\n",
    "    metrics = smote_results[name]['metrics']\n",
    "    final_comparison.append({\n",
    "        'model_name': f\"{name} (SMOTE)\",\n",
    "        'auc': metrics['roc_auc'],\n",
    "        'f1': metrics['f1'],\n",
    "        'precision': metrics['precision'],\n",
    "        'recall': metrics['recall']\n",
    "    })\n",
    "\n",
    "# Tuned model (n·∫øu c√≥)\n",
    "if 'tuned_metrics' in locals():\n",
    "    final_comparison.append({\n",
    "        'model_name': f\"{best_model_name} (Tuned)\",\n",
    "        'auc': tuned_metrics['roc_auc'],\n",
    "        'f1': tuned_metrics['f1'],\n",
    "        'precision': tuned_metrics['precision'],\n",
    "        'recall': tuned_metrics['recall']\n",
    "    })\n",
    "\n",
    "# T√¨m m√¥ h√¨nh t·ªët nh·∫•t d·ª±a tr√™n AUC\n",
    "best_overall = max(final_comparison, key=lambda x: x['auc'] if x['auc'] else 0)\n",
    "\n",
    "print(f\"üèÜ M√î H√åNH T·ªêT NH·∫§T: {best_overall['model_name']}\")\n",
    "print(f\"   AUC: {best_overall['auc']:.4f}\")\n",
    "print(f\"   F1:  {best_overall['f1']:.4f}\")\n",
    "print(f\"   Precision: {best_overall['precision']:.4f}\")\n",
    "print(f\"   Recall: {best_overall['recall']:.4f}\")\n",
    "\n",
    "# L∆∞u m√¥ h√¨nh v√† metadata\n",
    "import os\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# X√°c ƒë·ªãnh m√¥ h√¨nh ƒë·ªÉ l∆∞u\n",
    "if 'Tuned' in best_overall['model_name']:\n",
    "    model_to_save = best_final_model\n",
    "    data_version = 'original'\n",
    "elif 'SMOTE' in best_overall['model_name']:\n",
    "    # Train l·∫°i m√¥ h√¨nh t·ªët nh·∫•t v·ªõi SMOTE data\n",
    "    model_name_clean = best_overall['model_name'].replace(' (SMOTE)', '')\n",
    "    model_to_save = models[model_name_clean]\n",
    "    model_to_save.fit(X_train_smote, y_train_smote)\n",
    "    data_version = 'smote'\n",
    "else:\n",
    "    model_name_clean = best_overall['model_name'].replace(' (Original)', '')\n",
    "    model_to_save = trained_models[model_name_clean]\n",
    "    data_version = 'original'\n",
    "\n",
    "# L∆∞u m√¥ h√¨nh\n",
    "model_info = {\n",
    "    'model': model_to_save,\n",
    "    'model_name': best_overall['model_name'],\n",
    "    'data_version': data_version,\n",
    "    'metrics': {\n",
    "        'auc': best_overall['auc'],\n",
    "        'f1': best_overall['f1'],\n",
    "        'precision': best_overall['precision'],\n",
    "        'recall': best_overall['recall']\n",
    "    },\n",
    "    'feature_names': data_original['feature_names'],\n",
    "    'scaler': data_original['scaler']\n",
    "}\n",
    "\n",
    "with open('../models/best_stroke_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_info, f)\n",
    "\n",
    "print(f\"\\n‚úÖ ƒê√£ l∆∞u m√¥ h√¨nh t·∫°i: ../models/best_stroke_model.pkl\")\n",
    "\n",
    "# L∆∞u t·∫•t c·∫£ k·∫øt qu·∫£\n",
    "all_results = {\n",
    "    'original_results': results,\n",
    "    'smote_results': smote_results,\n",
    "    'best_model_info': model_info,\n",
    "    'comparison_data': final_comparison\n",
    "}\n",
    "\n",
    "with open('../results/modeling_results.pkl', 'wb') as f:\n",
    "    pickle.dump(all_results, f)\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u t·∫•t c·∫£ k·∫øt qu·∫£ t·∫°i: ../results/modeling_results.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. T√≥m T·∫Øt K·∫øt Qu·∫£ Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"üìã T√ìM T·∫ÆT QU√Å TR√åNH MODELING:\n",
    "\n",
    "ü§ñ C√ÅC M√î H√åNH ƒê√É TH·ª∞C NGHI·ªÜM:\n",
    "1. ‚úÖ Logistic Regression\n",
    "2. ‚úÖ Random Forest  \n",
    "3. ‚úÖ Gradient Boosting\n",
    "4. ‚úÖ XGBoost\n",
    "5. ‚úÖ LightGBM\n",
    "6. ‚úÖ SVM\n",
    "7. ‚úÖ K-Nearest Neighbors\n",
    "8. ‚úÖ Naive Bayes\n",
    "9. ‚úÖ Decision Tree\n",
    "10. ‚úÖ AdaBoost\n",
    "\n",
    "üìä PH∆Ø∆†NG PH√ÅP ƒê√ÅNH GI√Å:\n",
    "- ‚úÖ 5-fold Cross Validation\n",
    "- ‚úÖ Multiple metrics: AUC, F1, Precision, Recall, Accuracy\n",
    "- ‚úÖ ROC Curves v√† Precision-Recall Curves\n",
    "- ‚úÖ Confusion Matrix analysis\n",
    "- ‚úÖ Feature Importance analysis\n",
    "\n",
    "üîß TECHNIQUES ƒê√É √ÅP D·ª§NG:\n",
    "- ‚úÖ Hyperparameter tuning v·ªõi RandomizedSearchCV\n",
    "- ‚úÖ So s√°nh Original vs SMOTE data\n",
    "- ‚úÖ Feature importance analysis cho tree-based models\n",
    "- ‚úÖ Comprehensive model comparison\n",
    "\n",
    "üèÜ K·∫æT QU·∫¢ CU·ªêI C√ôNG:\n",
    "- M√¥ h√¨nh t·ªët nh·∫•t: {}\n",
    "- AUC Score: {:.4f}\n",
    "- F1 Score: {:.4f}\n",
    "- Precision: {:.4f}\n",
    "- Recall: {:.4f}\n",
    "\n",
    "üíæ FILES ƒê√É L∆ØU:\n",
    "- best_stroke_model.pkl: M√¥ h√¨nh t·ªët nh·∫•t + metadata\n",
    "- modeling_results.pkl: T·∫•t c·∫£ k·∫øt qu·∫£ th·ª±c nghi·ªám\n",
    "\n",
    "üöÄ S·∫¥N S√ÄNG CHO DEPLOYMENT!\n",
    "\"\"\".format(\n",
    "    best_overall['model_name'],\n",
    "    best_overall['auc'],\n",
    "    best_overall['f1'],\n",
    "    best_overall['precision'],\n",
    "    best_overall['recall']\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
